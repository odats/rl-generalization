{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjMtg3Hak1Kf"
   },
   "source": [
    "# Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wbcJ62iGk__0",
    "outputId": "e2c23c81-98a1-46bc-f296-723d6ebabd75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'baselines' already exists and is not an empty directory.\n",
      "fatal: destination path 'rl-generalization' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/baselines.git\n",
    "!git clone https://github.com/sunblaze-ucb/rl-generalization.git\n",
    "  \n",
    "# pip install git+https://github.com/openai/baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "JzflIdQtmf51",
    "outputId": "a660a609-fe11-4d29-82f4-7602fa50af4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.3)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (2.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.15.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BP9x10kLLZ8f",
    "outputId": "62f8200a-1f17-4a4e-db85-01c365f9fe9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "import sys\n",
    "sys.path.append('rl-generalization')\n",
    "sys.path.append('baselines')\n",
    "import sunblaze_envs\n",
    "import baselines\n",
    "\n",
    "from baselines.common.tf_util import get_session\n",
    "\n",
    "def clean_session_fix():\n",
    "    get_session().close()\n",
    "    tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wso9OB3XMHgJ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from baselines import deepq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HMHwEPqMNBY"
   },
   "outputs": [],
   "source": [
    "#%run baselines/baselines/deepq/experiments/train_pong.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fiHGt-2mktPN"
   },
   "source": [
    "# Train and Play methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qq7b_POWWEH1"
   },
   "outputs": [],
   "source": [
    "def callback(lcl, _glb):\n",
    "    # stop training if reward exceeds 199\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 199\n",
    "    return is_solved\n",
    "\n",
    "\n",
    "def train_cartpole(env, model_name):\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        network='mlp',\n",
    "        lr=1e-3,\n",
    "        total_timesteps=1000000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        #checkpoint_path=\"cartpole\",\n",
    "        print_freq=100,\n",
    "        callback=callback\n",
    "    )\n",
    "    \n",
    "    print(\"Saving model to\", model_name)\n",
    "    act.save(model_name)\n",
    "    \n",
    "    return model_name\n",
    "    \n",
    "    \n",
    "def play_cartpole(env, model_name):\n",
    "    act = deepq.learn(env, network='mlp', total_timesteps=0, load_path=model_name)\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(100):\n",
    "        obs, done = env.reset(), False\n",
    "        episode_rew = 0\n",
    "        while not done:\n",
    "            obs, rew, done, _ = env.step(act(obs[None])[0])\n",
    "            episode_rew += rew\n",
    "        rewards.append(episode_rew)\n",
    "        \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46qEEfoekeNS"
   },
   "source": [
    "# Train on Deterministic. Test on ALL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12413
    },
    "colab_type": "code",
    "id": "7sZwlLPdZ0I8",
    "outputId": "1f88e011-faaa-43ed-a92e-305542b995cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From baselines/baselines/common/models.py:50: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rl-generalization/sunblaze_envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2019-05-05-11-24-52-900074\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 21.6     |\n",
      "| steps                   | 2134     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 24       |\n",
      "| steps                   | 4536     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 24.2     |\n",
      "| steps                   | 6954     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 21.8     |\n",
      "| steps                   | 9138     |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> 19.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 20.8     |\n",
      "| steps                   | 11215    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 23.1     |\n",
      "| steps                   | 13522    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 28.3     |\n",
      "| steps                   | 16348    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 31.6     |\n",
      "| steps                   | 19503    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 19.8 -> 30.1\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 78       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 21.9     |\n",
      "| steps                   | 21692    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 18.9     |\n",
      "| steps                   | 23581    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 74       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 20.6     |\n",
      "| steps                   | 25641    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 72       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 23       |\n",
      "| steps                   | 27941    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 70       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 24.1     |\n",
      "| steps                   | 30353    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 68       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 22.7     |\n",
      "| steps                   | 32622    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 20.6     |\n",
      "| steps                   | 34678    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 62       |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 31.6     |\n",
      "| steps                   | 37838    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 60       |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 27       |\n",
      "| steps                   | 40536    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 21.1     |\n",
      "| steps                   | 42646    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 23.2     |\n",
      "| steps                   | 44965    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 49368    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 30.1 -> 45.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 49       |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 20.4     |\n",
      "| steps                   | 51405    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 44.5     |\n",
      "| steps                   | 55857    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 45.7 -> 71.1\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 38       |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 71.6     |\n",
      "| steps                   | 63017    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 35.9     |\n",
      "| steps                   | 66608    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 32       |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 22.4     |\n",
      "| steps                   | 68845    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 25       |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 67.9     |\n",
      "| steps                   | 75636    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 20       |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 54.2     |\n",
      "| steps                   | 81059    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 15       |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 51.9     |\n",
      "| steps                   | 86246    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 11       |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 41.1     |\n",
      "| steps                   | 90356    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 95.1     |\n",
      "| steps                   | 99870    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 71.1 -> 94.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 98.4     |\n",
      "| steps                   | 109714   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 94.8 -> 97.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 79.8     |\n",
      "| steps                   | 117694   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 79.5     |\n",
      "| steps                   | 125640   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 47.9     |\n",
      "| steps                   | 130430   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 47.4     |\n",
      "| steps                   | 135166   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 131      |\n",
      "| steps                   | 148258   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 97.0 -> 144.1\n",
      "Saving model due to mean reward increase: 144.1 -> 154.5\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 127      |\n",
      "| steps                   | 160922   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 89.2     |\n",
      "| steps                   | 169841   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 110      |\n",
      "| steps                   | 180788   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 121      |\n",
      "| steps                   | 192893   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 160      |\n",
      "| steps                   | 208851   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 154.5 -> 163.6\n",
      "Saving model due to mean reward increase: 163.6 -> 184.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 195      |\n",
      "| steps                   | 228307   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 184.6 -> 194.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 178      |\n",
      "| steps                   | 246065   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 146      |\n",
      "| steps                   | 260682   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 185      |\n",
      "| steps                   | 279207   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 178      |\n",
      "| steps                   | 297015   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 179      |\n",
      "| steps                   | 314935   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 132      |\n",
      "| steps                   | 328153   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 188      |\n",
      "| steps                   | 347005   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 127      |\n",
      "| steps                   | 359723   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | 174      |\n",
      "| steps                   | 377097   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5200     |\n",
      "| mean 100 episode reward | 64.7     |\n",
      "| steps                   | 383570   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5300     |\n",
      "| mean 100 episode reward | 161      |\n",
      "| steps                   | 399647   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5400     |\n",
      "| mean 100 episode reward | 121      |\n",
      "| steps                   | 411757   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5500     |\n",
      "| mean 100 episode reward | 101      |\n",
      "| steps                   | 421819   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5600     |\n",
      "| mean 100 episode reward | 123      |\n",
      "| steps                   | 434164   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5700     |\n",
      "| mean 100 episode reward | 75.8     |\n",
      "| steps                   | 441745   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5800     |\n",
      "| mean 100 episode reward | 55       |\n",
      "| steps                   | 447247   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5900     |\n",
      "| mean 100 episode reward | 65.4     |\n",
      "| steps                   | 453789   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6000     |\n",
      "| mean 100 episode reward | 111      |\n",
      "| steps                   | 464849   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6100     |\n",
      "| mean 100 episode reward | 132      |\n",
      "| steps                   | 478005   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6200     |\n",
      "| mean 100 episode reward | 117      |\n",
      "| steps                   | 489680   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6300     |\n",
      "| mean 100 episode reward | 142      |\n",
      "| steps                   | 503857   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6400     |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 517159   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6500     |\n",
      "| mean 100 episode reward | 57.6     |\n",
      "| steps                   | 522923   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6600     |\n",
      "| mean 100 episode reward | 68       |\n",
      "| steps                   | 529723   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6700     |\n",
      "| mean 100 episode reward | 75.1     |\n",
      "| steps                   | 537232   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6800     |\n",
      "| mean 100 episode reward | 51.7     |\n",
      "| steps                   | 542399   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6900     |\n",
      "| mean 100 episode reward | 53.5     |\n",
      "| steps                   | 547747   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7000     |\n",
      "| mean 100 episode reward | 61.2     |\n",
      "| steps                   | 553864   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7100     |\n",
      "| mean 100 episode reward | 118      |\n",
      "| steps                   | 565627   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7200     |\n",
      "| mean 100 episode reward | 145      |\n",
      "| steps                   | 580102   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7300     |\n",
      "| mean 100 episode reward | 96       |\n",
      "| steps                   | 589704   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7400     |\n",
      "| mean 100 episode reward | 104      |\n",
      "| steps                   | 600051   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7500     |\n",
      "| mean 100 episode reward | 120      |\n",
      "| steps                   | 612031   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7600     |\n",
      "| mean 100 episode reward | 53.8     |\n",
      "| steps                   | 617408   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7700     |\n",
      "| mean 100 episode reward | 36.1     |\n",
      "| steps                   | 621021   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7800     |\n",
      "| mean 100 episode reward | 30.4     |\n",
      "| steps                   | 624063   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7900     |\n",
      "| mean 100 episode reward | 49.6     |\n",
      "| steps                   | 629028   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8000     |\n",
      "| mean 100 episode reward | 79.7     |\n",
      "| steps                   | 636994   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8100     |\n",
      "| mean 100 episode reward | 93.4     |\n",
      "| steps                   | 646338   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8200     |\n",
      "| mean 100 episode reward | 93.7     |\n",
      "| steps                   | 655712   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8300     |\n",
      "| mean 100 episode reward | 79.2     |\n",
      "| steps                   | 663633   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8400     |\n",
      "| mean 100 episode reward | 58.8     |\n",
      "| steps                   | 669508   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8500     |\n",
      "| mean 100 episode reward | 57.2     |\n",
      "| steps                   | 675229   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8600     |\n",
      "| mean 100 episode reward | 80.7     |\n",
      "| steps                   | 683297   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8700     |\n",
      "| mean 100 episode reward | 80.9     |\n",
      "| steps                   | 691383   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8800     |\n",
      "| mean 100 episode reward | 92.6     |\n",
      "| steps                   | 700648   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8900     |\n",
      "| mean 100 episode reward | 83.8     |\n",
      "| steps                   | 709031   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9000     |\n",
      "| mean 100 episode reward | 98       |\n",
      "| steps                   | 718828   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9100     |\n",
      "| mean 100 episode reward | 103      |\n",
      "| steps                   | 729102   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9200     |\n",
      "| mean 100 episode reward | 65.9     |\n",
      "| steps                   | 735693   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9300     |\n",
      "| mean 100 episode reward | 74.5     |\n",
      "| steps                   | 743147   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9400     |\n",
      "| mean 100 episode reward | 108      |\n",
      "| steps                   | 753967   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9500     |\n",
      "| mean 100 episode reward | 112      |\n",
      "| steps                   | 765156   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9600     |\n",
      "| mean 100 episode reward | 142      |\n",
      "| steps                   | 779316   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9700     |\n",
      "| mean 100 episode reward | 113      |\n",
      "| steps                   | 790640   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9800     |\n",
      "| mean 100 episode reward | 73.6     |\n",
      "| steps                   | 797998   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9900     |\n",
      "| mean 100 episode reward | 87.8     |\n",
      "| steps                   | 806781   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10000    |\n",
      "| mean 100 episode reward | 83.4     |\n",
      "| steps                   | 815124   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10100    |\n",
      "| mean 100 episode reward | 107      |\n",
      "| steps                   | 825792   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10200    |\n",
      "| mean 100 episode reward | 127      |\n",
      "| steps                   | 838479   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10300    |\n",
      "| mean 100 episode reward | 122      |\n",
      "| steps                   | 850686   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10400    |\n",
      "| mean 100 episode reward | 148      |\n",
      "| steps                   | 865474   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10500    |\n",
      "| mean 100 episode reward | 90.8     |\n",
      "| steps                   | 874552   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10600    |\n",
      "| mean 100 episode reward | 84.1     |\n",
      "| steps                   | 882961   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10700    |\n",
      "| mean 100 episode reward | 48.7     |\n",
      "| steps                   | 887835   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10800    |\n",
      "| mean 100 episode reward | 106      |\n",
      "| steps                   | 898456   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10900    |\n",
      "| mean 100 episode reward | 148      |\n",
      "| steps                   | 913262   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11000    |\n",
      "| mean 100 episode reward | 136      |\n",
      "| steps                   | 926817   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11100    |\n",
      "| mean 100 episode reward | 137      |\n",
      "| steps                   | 940526   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11200    |\n",
      "| mean 100 episode reward | 107      |\n",
      "| steps                   | 951232   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11300    |\n",
      "| mean 100 episode reward | 71.1     |\n",
      "| steps                   | 958339   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11400    |\n",
      "| mean 100 episode reward | 84.5     |\n",
      "| steps                   | 966789   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11500    |\n",
      "| mean 100 episode reward | 117      |\n",
      "| steps                   | 978520   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 11600    |\n",
      "| mean 100 episode reward | 145      |\n",
      "| steps                   | 993044   |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: 194.6\n",
      "Saving model to cartpole_model_d.pkl\n"
     ]
    }
   ],
   "source": [
    "model_name = train_cartpole(env = sunblaze_envs.make('SunblazeCartPole-v0'), \n",
    "                            model_name = 'cartpole_model_d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "iPRop53zjghQ",
    "outputId": "77462c5e-e2bb-46c7-cdf8-5ba567681ea8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rl-generalization/sunblaze_envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from cartpole_model_d.pkl\n",
      "DD 184.36\n",
      "Loaded model from cartpole_model_d.pkl\n",
      "DR 134.96\n",
      "Loaded model from cartpole_model_d.pkl\n",
      "DE 64.25\n"
     ]
    }
   ],
   "source": [
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPole-v0'), model_name)\n",
    "print('DD', avg_reward)\n",
    "\n",
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPoleRandomNormal-v0'), model_name)\n",
    "print('DR', avg_reward)\n",
    "\n",
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPoleRandomExtreme-v0'), model_name)\n",
    "print('DE', avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vk5nUH3kKw2"
   },
   "source": [
    "# Train on Stochastic. Test on ALL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4440
    },
    "colab_type": "code",
    "id": "WctCgUaokCdJ",
    "outputId": "285fcc0b-7ab8-4ffd-9e84-1f34680401aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rl-generalization/sunblaze_envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 23.3     |\n",
      "| steps                   | 2306     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 24.4     |\n",
      "| steps                   | 4741     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 23.6     |\n",
      "| steps                   | 7104     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 26.1     |\n",
      "| steps                   | 9711     |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> 25.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 27.9     |\n",
      "| steps                   | 12499    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 24       |\n",
      "| steps                   | 14903    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 27.2     |\n",
      "| steps                   | 17628    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 25.6 -> 33.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 34.9     |\n",
      "| steps                   | 21117    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 32.4     |\n",
      "| steps                   | 24361    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 23.6     |\n",
      "| steps                   | 26717    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 71       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 25.1     |\n",
      "| steps                   | 29229    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 68       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 33.6     |\n",
      "| steps                   | 32592    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 65       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 25.5     |\n",
      "| steps                   | 35140    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 40.9     |\n",
      "| steps                   | 39233    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 33.0 -> 44.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 33.8     |\n",
      "| steps                   | 42613    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 54       |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 34.8     |\n",
      "| steps                   | 46093    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 31       |\n",
      "| steps                   | 49188    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 66.6     |\n",
      "| steps                   | 55849    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 44.7 -> 66.5\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 38       |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 67.9     |\n",
      "| steps                   | 62636    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 33       |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 51.8     |\n",
      "| steps                   | 67821    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 66.5 -> 66.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 28       |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 49       |\n",
      "| steps                   | 72722    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 25       |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 35.2     |\n",
      "| steps                   | 76241    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 16       |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 87.2     |\n",
      "| steps                   | 84956    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 66.6 -> 67.2\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 10       |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 64       |\n",
      "| steps                   | 91357    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 66.2     |\n",
      "| steps                   | 97975    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 67.2 -> 75.4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 93.4     |\n",
      "| steps                   | 107316   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 75.4 -> 102.7\n",
      "Saving model due to mean reward increase: 102.7 -> 128.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 129      |\n",
      "| steps                   | 120185   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 127      |\n",
      "| steps                   | 132877   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 125      |\n",
      "| steps                   | 145368   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 99       |\n",
      "| steps                   | 155266   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 128.8 -> 163.5\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 156      |\n",
      "| steps                   | 170918   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 81.1     |\n",
      "| steps                   | 179027   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 137      |\n",
      "| steps                   | 192736   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 141      |\n",
      "| steps                   | 206829   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 179      |\n",
      "| steps                   | 224740   |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 163.5 -> 181.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 238079   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 126      |\n",
      "| steps                   | 250645   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 102      |\n",
      "| steps                   | 260893   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 143      |\n",
      "| steps                   | 275238   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 187      |\n",
      "| steps                   | 293954   |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: 181.8\n",
      "Saving model to cartpole_model_r.pkl\n"
     ]
    }
   ],
   "source": [
    "clean_session_fix()\n",
    "model_name_r = train_cartpole(env = sunblaze_envs.make('SunblazeCartPoleRandomNormal-v0'), \n",
    "                              model_name = 'cartpole_model_r.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "pWcDA-OzkFzb",
    "outputId": "22e669cd-0805-4eb5-eb7e-ebf4b1014884"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rl-generalization/sunblaze_envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from cartpole_model_r.pkl\n",
      "RD 200.0\n",
      "Loaded model from cartpole_model_r.pkl\n",
      "RR 200.0\n",
      "Loaded model from cartpole_model_r.pkl\n",
      "RE 172.32\n"
     ]
    }
   ],
   "source": [
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPole-v0'), model_name_r)\n",
    "print('RD', avg_reward)\n",
    "\n",
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPoleRandomNormal-v0'), model_name_r)\n",
    "print('RR', avg_reward)\n",
    "\n",
    "clean_session_fix()\n",
    "avg_reward = play_cartpole(sunblaze_envs.make('SunblazeCartPoleRandomExtreme-v0'), model_name_r)\n",
    "print('RE', avg_reward)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQN for Sunblaze CartPole (DRE), openai baseline.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
